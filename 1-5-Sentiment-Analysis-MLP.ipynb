{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use the concepts of MLP and Backpropagation to build a model capable of classifying the sentiments of a review as either positive(1) or negative(0). The datastes we are going to use are provided in Udacity Notebooks and the solution is totally based on the approach by [Andrew Trask](https://github.com/udacity/deep-learning-v2-pytorch/blob/master/sentiment-analysis-network/Sentiment_Classification_Solutions.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very first thing we would like to do is evaluate our dataset. For that let's import some useful packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0) # Setting seed to get same random number pattern every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Steps\n",
    "When we look at the labels and its corresponding label, we see that the last character of the label is newline (\\n) which\n",
    "is not the part of the review per se, so will ignore newline character from both reviews and labels. Also we will change every character to uppercase so Bad, BAD, bad all be BAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reviews.txt\", \"r\") as reviews_dataset:\n",
    "    reviews = reviews_dataset.readlines()\n",
    "    \n",
    "with open(\"labels.txt\", \"r\") as labels_dataset:\n",
    "    labels = labels_dataset.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \\n'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive\\n'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "## While reading reviews and labels, we only take characters from 1st to second last position.\n",
    "## i.e we ignore the last term.\n",
    "\n",
    "with open(\"reviews.txt\", \"r\") as reviews_dataset:\n",
    "    reviews = list(map(lambda x:x[:-1].upper(), reviews_dataset.readlines()))\n",
    "\n",
    "with open(\"labels.txt\", \"r\") as labels_dataset:\n",
    "    labels = list(map(lambda x:x[:-1].upper(), labels_dataset.readlines()))\n",
    "    \n",
    "    \n",
    "assert(len(reviews) == len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BROMWELL HIGH IS A CARTOON COMEDY . IT RAN AT THE SAME TIME AS SOME OTHER PROGRAMS ABOUT SCHOOL LIFE  SUCH AS  TEACHERS  . MY   YEARS IN THE TEACHING PROFESSION LEAD ME TO BELIEVE THAT BROMWELL HIGH  S SATIRE IS MUCH CLOSER TO REALITY THAN IS  TEACHERS  . THE SCRAMBLE TO SURVIVE FINANCIALLY  THE INSIGHTFUL STUDENTS WHO CAN SEE RIGHT THROUGH THEIR PATHETIC TEACHERS  POMP  THE PETTINESS OF THE WHOLE SITUATION  ALL REMIND ME OF THE SCHOOLS I KNEW AND THEIR STUDENTS . WHEN I SAW THE EPISODE IN WHICH A STUDENT REPEATEDLY TRIED TO BURN DOWN THE SCHOOL  I IMMEDIATELY RECALLED . . . . . . . . . AT . . . . . . . . . . HIGH . A CLASSIC LINE INSPECTOR I  M HERE TO SACK ONE OF YOUR TEACHERS . STUDENT WELCOME TO BROMWELL HIGH . I EXPECT THAT MANY ADULTS OF MY AGE THINK THAT BROMWELL HIGH IS FAR FETCHED . WHAT A PITY THAT IT ISN  T   '"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing data to numbers\n",
    "A neural network only deals with numbers. No matter if we want to work on image, video, texts, speech the input data and label must be changed into numbers.\n",
    "\n",
    "#### For reviews\n",
    "\n",
    "\"THIS MOVIE ROCKS\" should be changed into somethng like 118, 1338, 383 or something efficient than that. \n",
    "\n",
    "One way to do is\n",
    "one-hot encode the reviews. That is, we create a vocabulary of the words of the reveiws and arrange them in some pattern. When a review arrives we put a **1** only in the place where the reviews and vocabulary matches and **0** everywhere else. The problem with this is, we will end up with very large input sequence, say we have a vocabulary of 2 million words, then we need a input layer with 2 million neurons which will be very resource hungry. Plus, most of the inputs will be 0s so it will not be efficient.\n",
    "\n",
    "Another approach which is also less eficient but will work satisfactory for experimental purposes is First we will create a vocabulary of all the words in the review, and assign them some numbers. i.e every words will get some distinct number. The problem with this approach is the confusion in the size of the input layer. There is not aggrement on the size of the reviews. Some reviews are short while some may be very long. To solve this, we can use a fixed size input layer and truncate reveiw character outside of our fixed size input. But this will result in the loss of information. Another problem with this approach is the difference in value of the words. If we choose this method, a word say \"back\" will have a value of 2, while a word \"every\" may have a value of 245393. Since neural network is all about numbers. There is no valid reason to have such a difference in the value of the word \"back\" and \"every\". A neural network will have to do computation and such a invalid difference in the representation of the dataset will cost us badly.\n",
    "\n",
    "The approach of using a fixed input size is quite odd. So, If we could manage to remove the unnecessary computation with **0s** in the first approach it could be a way better choice than other. We will  encode the review words with respect to the lenght of vocabulary. Then will just select the corresponding weights from weight matrix and sum them up. Doing this is similary to doing a very long vector-matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.61084403 2.14088992 2.01623124]\n"
     ]
    }
   ],
   "source": [
    "## A naive approach\n",
    "np.random.seed(0)\n",
    "input_review = np.array([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0])\n",
    "hidden_size = 3\n",
    "\n",
    "weights = np.random.random((len(input_review), hidden_size))\n",
    "output = weights.T.dot(input_review)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.61084403 2.14088992 2.01623124]\n"
     ]
    }
   ],
   "source": [
    "## A better approach\n",
    "np.random.seed(0)\n",
    "input_review = np.array([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0])\n",
    "\n",
    "input_words_index = np.array([3, 9, 12, 15])\n",
    "hidden_size = 3\n",
    "\n",
    "weights = np.random.random((18, hidden_size))\n",
    "\n",
    "output = np.sum(weights[input_words_index], axis=0)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the output of both approach is same. The second one is better because we don't have to deal with unnecessary multiplication of **0s** we just select the corresponding row where the input is present and sum them up. Doing this is mathematically equivalent of doing a vector-matrix multiplication.\n",
    "\n",
    "### For Labels\n",
    "For label we will change \"positive\" to 1 and negative to 0.\n",
    "\n",
    "Now Let's encode our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(map(lambda x:set(x.split(\" \")), reviews))\n",
    "\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        if(len(word)>0):\n",
    "            vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "\n",
    "input_dataset = list()\n",
    "for sent in tokens:\n",
    "    sent_indices = list()\n",
    "    for word in sent:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(list(set(sent_indices)))\n",
    "\n",
    "target_dataset = list()\n",
    "for label in labels:\n",
    "    if label == \"POSITIVE\":\n",
    "        target_dataset.append(1)\n",
    "    else:\n",
    "        target_dataset.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37889, 69124, 20489, 27145, 49163, 34346, 59950, 13362, 44086, 29760, 33345, 2114, 42564, 37956, 46149, 38479, 62039, 21596, 26717, 26205, 16990, 33379, 53859, 47214, 38007, 35968, 67201, 66184, 19595, 32909, 31893, 41625, 36003, 62638, 43700, 4277, 15035, 28351, 37060, 20176, 5328, 6872, 53473, 71401, 40170, 14069, 40697, 3838, 29439, 10497, 47363, 48927, 27430, 53031, 61741, 20300, 60752, 67922, 35674, 41311, 5481, 61298, 60282, 5503, 37254, 66442, 1425, 58259, 55191, 15275, 70063, 63411, 25526, 10167, 41917, 42429, 51137, 12738, 29633, 33218, 71628, 14293, 15318, 59352, 23512, 24028, 14829, 69104, 57842, 22515, 59382, 72187] 1\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "# First reveiw and its corresponding target\n",
    "print(input_dataset[0], target_dataset[0])\n",
    "\n",
    "print(len(input_dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split the dataset into train and testing set\n",
    "train_test_split = 0.6 ## 60% of the review will be used for training and rest of 40% for testing\n",
    "\n",
    "train_test_split_index = int(len(input_dataset) * 0.9)\n",
    "\n",
    "train_review_dataset = input_dataset[0:train_test_split_index]\n",
    "test_review_dataset = input_dataset[train_test_split_index:]\n",
    "\n",
    "train_label = target_dataset[0:train_test_split_index]\n",
    "test_label = target_dataset[train_test_split_index:]\n",
    "\n",
    "assert(len(train_review_dataset) == len(train_label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of our neural network is very simple. At the first layer we will select corresponing rows of weight matrix instead of doing a very costly vector-matrix multiplication. The next layer will consist of 100 neurons and the final layer will have single neuron.\n",
    "\n",
    "After calculating the output of final layer we will use backpropagation algorithm to update weights.\n",
    "\n",
    "To calculate the accuracy we will find the difference between our prediction i.e layer_2 and correct prediction. If the difference between them is less than 0.5 then we will assume that our prediction is correct.\n",
    "\n",
    "We will use sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's define an activation function\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid2deriv(x):\n",
    "    return x * (1 - x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = len(vocab)\n",
    "num_hidden = 100\n",
    "num_output = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74073, 100)\n",
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "weights_0_1 = 0.02 * np.random.random((num_input, num_hidden)) - 0.01\n",
    "print(weights_0_1.shape)\n",
    "weights_1_2 = 0.02 * np.random.random((num_hidden, num_output)) - 0.01\n",
    "print(weights_1_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3\n",
      "Train Accuracy: 76.14% Train Error: [0.22116486]\n",
      "Test Accuracy: 77.60% Test Error: [0.19269695]\n",
      "Iteration: 6\n",
      "Train Accuracy: 86.31% Train Error: [0.10339513]\n",
      "Test Accuracy: 85.12% Test Error: [0.10943309]\n",
      "Iteration: 9\n",
      "Train Accuracy: 89.50% Train Error: [0.07871256]\n",
      "Test Accuracy: 85.96% Test Error: [0.09721294]\n",
      "Iteration: 12\n",
      "Train Accuracy: 91.54% Train Error: [0.06512396]\n",
      "Test Accuracy: 86.88% Test Error: [0.0939214]\n",
      "Iteration: 15\n",
      "Train Accuracy: 93.01% Train Error: [0.05554205]\n",
      "Test Accuracy: 87.32% Test Error: [0.09355907]\n",
      "Iteration: 18\n",
      "Train Accuracy: 94.18% Train Error: [0.04800082]\n",
      "Test Accuracy: 87.60% Test Error: [0.09429767]\n",
      "Iteration: 21\n",
      "Train Accuracy: 95.01% Train Error: [0.04173533]\n",
      "Test Accuracy: 87.16% Test Error: [0.09546391]\n",
      "Iteration: 24\n",
      "Train Accuracy: 95.75% Train Error: [0.03640588]\n",
      "Test Accuracy: 86.84% Test Error: [0.09677049]\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 24\n",
    "lr = 0.001\n",
    "training_losses = list()\n",
    "testing_losses = list()\n",
    "\n",
    "for itera in range(1, num_iterations + 1):\n",
    "    error = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for idx in range(len(train_review_dataset)):\n",
    "        layer_1 = np.sum(weights_0_1[train_review_dataset[idx]], axis=0)\n",
    "        layer_1 = sigmoid(layer_1)\n",
    "        layer_2 = weights_1_2.T.dot(layer_1)\n",
    "        layer_2 = sigmoid(layer_2)\n",
    "        \n",
    "        error = error + (layer_2 - train_label[idx]) ** 2\n",
    "        \n",
    "        layer_2_delta = layer_2 - train_label[idx]\n",
    "        layer_1_delta = weights_1_2.dot(layer_2_delta) * sigmoid2deriv(layer_1)\n",
    "        \n",
    "        delta_weights_1_2 = layer_1.reshape(100,1).dot(layer_2_delta.reshape(1, 1))\n",
    "        delta_weights_0_1 = layer_1_delta\n",
    "    \n",
    "        weights_1_2 = weights_1_2 - lr * delta_weights_1_2\n",
    "\n",
    "        weights_0_1[input_dataset[idx]] = weights_0_1[train_review_dataset[idx]] - lr * layer_1_delta\n",
    "        \n",
    "        correct = correct + int(np.abs(layer_2_delta) < 0.5)\n",
    "    \n",
    "    \n",
    "    if itera % 3 == 0:\n",
    "        test_error = 0\n",
    "        test_correct = 0\n",
    "\n",
    "        for idx in range(len(test_review_dataset)):\n",
    "\n",
    "            layer_1 = np.sum(weights_0_1[test_review_dataset[idx]], axis=0)\n",
    "            layer_1 = sigmoid(layer_1)\n",
    "            layer_2 = weights_1_2.T.dot(layer_1)\n",
    "            layer_2 = sigmoid(layer_2)\n",
    "\n",
    "            test_error = test_error + (layer_2 - test_label[idx]) ** 2\n",
    "            test_correct = test_correct + int(np.abs(layer_2 - test_label[idx]) < 0.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Iteration: {}\".format(itera))\n",
    "        print(\"Train Accuracy: {:.2f}% Train Error: {}\".format((correct/len(train_review_dataset)) * 100,\n",
    "                                                           error/len(train_review_dataset)))\n",
    "        print(\"Test Accuracy: {:.2f}% Test Error: {}\".format((test_correct/len(test_review_dataset)) * 100,\n",
    "              test_error/len(test_review_dataset)))\n",
    "    \n",
    "    training_losses.append(error)\n",
    "    testing_losses.append(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f73e81bdc70>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_losses, \"r\", label=\"Train Loss\")\n",
    "plt.plot(testing_losses, \"b\", label=\"Test Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inference\n",
    "\n",
    "def predict_sentiment(review):\n",
    "    \n",
    "    sent_indices = list()\n",
    "    for word in review.split(\" \"):\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    layer_1 = np.sum(weights_0_1[sent_indices], axis=0)\n",
    "    layer_1 = sigmoid(layer_1)\n",
    "    layer_2 = weights_1_2.T.dot(layer_1)\n",
    "    layer_2 = sigmoid(layer_2)\n",
    "    \n",
    "    if np.abs(layer_2) > 0.5:\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Negative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Review with emphasisis on positive and negative words.\n",
    "\n",
    "test_reviews = [\n",
    "    \"plot of the movie was very good, i really like it\",\n",
    "    \"lovely flim\",\n",
    "    \"dumbest plot, bad movie\",\n",
    "    \"i do not like the movie, worst \",\n",
    "    \n",
    "    \"bad\",\n",
    "    \"good\",\n",
    "    \"Wonderful movie\",\n",
    "    \"film\",\n",
    "    \"worst movie of all time\",\n",
    "    \"Not Good\",\n",
    "    \"Not Bad\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLOT OF THE MOVIE WAS VERY GOOD, I REALLY LIKE IT : Positive\n",
      "LOVELY FLIM : Positive\n",
      "DUMBEST PLOT, BAD MOVIE : Negative\n",
      "I DO NOT LIKE THE MOVIE, WORST  : Negative\n",
      "BAD : Negative\n",
      "GOOD : Positive\n",
      "WONDERFUL MOVIE : Positive\n",
      "FILM : Positive\n",
      "WORST MOVIE OF ALL TIME : Negative\n",
      "NOT GOOD : Positive\n",
      "NOT BAD : Negative\n"
     ]
    }
   ],
   "source": [
    "for test_review in test_reviews:\n",
    "    test_review = test_review.upper()\n",
    "    print(\"{} : {}\".format(test_review, predict_sentiment(test_review)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our model was not able to understand \"Negation\" very well. It predicts \"NOT GOOD\" as positive and \"NOT BAD\" as Negative. The problem is every word is given a number and to counter that word we need another very powerful word. Example if we want a review to be positive then we need a lot of positive words and if we want a review to be negative then we need a lot of negative words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
