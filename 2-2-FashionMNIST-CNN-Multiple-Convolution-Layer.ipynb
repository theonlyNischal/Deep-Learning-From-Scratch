{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to use a  Convolution neural net to classify a [FashionMNIST Dataset](https://www.kaggle.com/zalando-research/fashionmnist). Unlike notebook 2-1, we will use multiple Convolution Layer.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For more details on CNN:\n",
    "\n",
    "\n",
    "[Convolutions and Backpropagations - Pavithra Solai](https://medium.com/@pavisj/convolutions-and-backpropagations-46026a8f5d2c)\n",
    "\n",
    "[Why do we rotate weights when computing the gradients in a convolution layer of a convolution network?](http://soumith.ch/ex/pages/2014/08/07/why-rotate-weights-convolution-gradient/)\n",
    "\n",
    "[A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285)\n",
    "\n",
    "[https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary Packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "## Read FashionMNIST Dataset\n",
    "dataframe = pd.read_csv(\"fashion-mnist_train.csv\")\n",
    "\n",
    "## Just using 700 images\n",
    "data = dataframe.values[:100]\n",
    "\n",
    "## Split labels and images\n",
    "labels = data[:, 0]\n",
    "images = data[:, 1:]\n",
    "\n",
    "## Normalize the image\n",
    "images = images/255\n",
    "\n",
    "## Splitting Training and Testing Set\n",
    "train_test_split = 0.85\n",
    "train_test_split_index = int(len(images) * train_test_split)\n",
    "training_images = images[:train_test_split_index ]\n",
    "testing_images = images[train_test_split_index: ]\n",
    "\n",
    "## One-hot encoding the labels\n",
    "\n",
    "one_hot_labels = []\n",
    "for label in labels:\n",
    "    base_label = np.zeros(10)\n",
    "    base_label[int(label)] = 1\n",
    "    one_hot_labels.append(base_label)\n",
    "\n",
    "labels = np.array(one_hot_labels)\n",
    "training_labels = labels[:train_test_split_index]\n",
    "testing_labels = labels[train_test_split_index: ]\n",
    "\n",
    "assert(len(training_images) == len(training_labels))\n",
    "assert(len(testing_images) == len(testing_labels))\n",
    "\n",
    "print(len(training_images))\n",
    "print(len(testing_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Labels name\n",
    "index_to_label_name = {\n",
    "    0: \"t-shirt\", 1: \"trousers\", 2: \"pullover\", 3: \"dress\", 4: \"coat\", 5: \"sandal\", 6: \"shirt\", 7: \"sneaker\",\n",
    "    8: \"bag\", 9: \"ankle boot\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdcf34449a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Sample Training Image with label\n",
    "\n",
    "img = training_images[7].reshape((28,28))\n",
    "label = index_to_label_name[np.argmax(training_labels[7])]\n",
    "plt.title(label)\n",
    "plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize weights\n",
    "\n",
    "input_image_height = input_image_width = 28\n",
    "input_image_depth = 1\n",
    "\n",
    "kernel_1_width = kernel_1_height = 3\n",
    "kernel_1_count = 16\n",
    "kernel_1_depth = input_image_depth\n",
    "\n",
    "kernel_1 = 0.2 * np.random.random((kernel_1_count , kernel_1_depth, kernel_1_height, kernel_1_width)) - 0.1\n",
    "\n",
    "layer_1_output_size = input_image_height - kernel_1_height + 1\n",
    "\n",
    "kernel_2_width = kernel_2_height = 3\n",
    "kernel_2_count = 16\n",
    "kernel_2_depth = kernel_1_count\n",
    "\n",
    "kernel_2 = 0.2 * np.random.random((kernel_2_count, kernel_2_depth, kernel_2_height, kernel_2_width)) - 0.1\n",
    "\n",
    "layer_2_output_size = layer_1_output_size - kernel_2_height + 1\n",
    "\n",
    "\n",
    "\n",
    "layer_3_size = 100\n",
    "\n",
    "weight_layer_2_layer_3 = 0.2 * np.random.random(\n",
    "    (kernel_2_count*layer_2_output_size*layer_2_output_size, layer_3_size)) - 0.1\n",
    "\n",
    "output_size = 10\n",
    "weight_layer_3_output = 0.2 * np.random.random((layer_3_size, output_size)) - 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Activation Function\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh2deriv(output):\n",
    "    return (1 - output**2)\n",
    "\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    return temp/np.sum(temp, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward Convolution\n",
    "## Takes input, kernel and returns output activation map\n",
    "def convolution(input_tensor, kernel):\n",
    "    kernel_height = kernel.shape[2]\n",
    "    kernel_width = kernel.shape[3]\n",
    "    output_tensor_height = input_tensor.shape[1] - kernel_height + 1\n",
    "    output_tensor_width = input_tensor.shape[2] - kernel_width + 1\n",
    "    \n",
    "    output = list()\n",
    "    \n",
    "    for kernel_count_idx in range(kernel.shape[0]):\n",
    "        select_kernel = kernel[kernel_count_idx]\n",
    "        \n",
    "        for row in range(output_tensor_height):\n",
    "            for col in range(output_tensor_width):\n",
    "                sum_kernel_sect = 0\n",
    "                for didx in range(input_tensor.shape[0]):\n",
    "                    sect = input_tensor[didx, row:row+kernel.shape[2], col:col+kernel.shape[3]]\n",
    "            \n",
    "                    sum_kernel_sect += np.sum(sect.dot(select_kernel[didx]))\n",
    "                output.append(sum_kernel_sect)                 \n",
    "\n",
    "    output = np.array(output)\n",
    "    output = tanh(output)\n",
    "    output = output.reshape((kernel.shape[0],output_tensor_height, output_tensor_width))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return output             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculates delta for the kernels\n",
    "## Takes deltas, previous layer and kernel in-between and returns deltas for kernel\n",
    "def weight_delta_convolution(input_tensor, delta_kernel_output, kernel):\n",
    "    delta_kernel = np.empty_like(kernel)\n",
    "    \n",
    "    for delta_kernel_output_idx in range(delta_kernel_output.shape[0]):\n",
    "        select_delta_kernel_output = delta_kernel_output[delta_kernel_output_idx]\n",
    "        \n",
    "        for row in range(input_tensor.shape[1] - select_delta_kernel_output.shape[0] + 1):\n",
    "            for col in range(input_tensor.shape[2] - select_delta_kernel_output.shape[1] + 1):\n",
    "                \n",
    "                for input_tensor_idx in range(input_tensor.shape[0]):\n",
    "                    sect = input_tensor[input_tensor_idx, row:row+select_delta_kernel_output.shape[0],\n",
    "                                        col:col+select_delta_kernel_output.shape[1]]\n",
    "                    delta_kernel[delta_kernel_output_idx, input_tensor_idx, row, col] = np.sum(\n",
    "                        sect.dot(select_delta_kernel_output)\n",
    "                    )\n",
    "    return delta_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itera: 1 Train Error: 1.590147 Accuracy: 8.2353%\n",
      "Itera: 2 Train Error: 1.026839 Accuracy: 22.3529%\n",
      "Itera: 3 Train Error: 0.876217 Accuracy: 36.4706%\n",
      "Itera: 4 Train Error: 0.797479 Accuracy: 43.5294%\n",
      "Itera: 5 Train Error: 0.756222 Accuracy: 47.0588%\n",
      "Itera: 6 Train Error: 0.727851 Accuracy: 48.2353%\n",
      "Itera: 7 Train Error: 0.704980 Accuracy: 51.7647%\n",
      "Itera: 8 Train Error: 0.678737 Accuracy: 54.1176%\n",
      "Itera: 9 Train Error: 0.660933 Accuracy: 56.4706%\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 10\n",
    "lr = 0.001\n",
    "train_losses = list() ## For plotting train loss \n",
    "test_losses = list()\n",
    "\n",
    "for itera in range(1, num_iterations+1):\n",
    "    \n",
    "    train_error = 0\n",
    "    train_correct = 0\n",
    "    \n",
    "    for img_idx in range(len(training_images)):\n",
    "        \n",
    "        img = training_images[img_idx]\n",
    "        label = training_labels[img_idx].reshape((10, 1))\n",
    "        \n",
    "        layer_0 = img.reshape((input_image_depth, input_image_height, input_image_width))\n",
    "        \n",
    "        layer_1 = convolution(layer_0, kernel_1)\n",
    "        \n",
    "#         layer_1_flattened = layer_1.reshape((-1, 1))\n",
    "        \n",
    "#         layer_2 = tanh(weight_layer_1_layer_2.T.dot(layer_1_flattened))\n",
    "        layer_2 = convolution(layer_1, kernel_2)\n",
    "        layer_2_flattened = layer_2.reshape((-1, 1))\n",
    "        \n",
    "        layer_3 = tanh(weight_layer_2_layer_3.T.dot(layer_2_flattened))\n",
    "    \n",
    "        \n",
    "        \n",
    "        final_output = weight_layer_3_output.T.dot(layer_3)\n",
    "        final_outut = softmax(final_output)\n",
    "        \n",
    "        train_error = train_error + np.sum((final_output - label)**2)\n",
    "        train_correct = train_correct + int(np.argmax(final_output) == np.argmax(label))\n",
    "        \n",
    "        \n",
    "        delta_final_output = final_output - label\n",
    "        delta_layer_3 = weight_layer_3_output.dot(delta_final_output) * tanh2deriv(layer_3)\n",
    "        delta_layer_2_flattened = weight_layer_2_layer_3.dot(delta_layer_3) * tanh2deriv(layer_2_flattened)\n",
    "        delta_layer_2 = delta_layer_2_flattened.reshape(layer_2.shape)\n",
    "        \n",
    "        padded_delta_layer_2 = np.pad(delta_layer_2, ([0,0], [2,2]\n",
    "                                                ,[2,2]))\n",
    "        \n",
    "        flipped_180_kernel_2 = np.rot90(kernel_2, 2)\n",
    "        \n",
    "        delta_layer_1 = convolution(padded_delta_layer_2, flipped_180_kernel_2)\n",
    "        \n",
    "        weight_layer_3_output = weight_layer_3_output - lr * layer_3.dot(delta_final_output.T)\n",
    "        weight_layer_2_layer_3 = weight_layer_2_layer_3 - lr * layer_2_flattened.dot(delta_layer_3.T)\n",
    "        \n",
    "        kernel_2 = kernel_2 - lr * weight_delta_convolution(\n",
    "            layer_1, delta_layer_2, kernel_2\n",
    "        )\n",
    "        \n",
    "        kernel_1 = kernel_1 - lr * weight_delta_convolution(\n",
    "            layer_0, delta_layer_1, kernel_1\n",
    "        )\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Itera: {} Train Error: {:4f} Accuracy: {:.4f}%\".format(itera, \n",
    "                    train_error/len(training_images), (train_correct/len(training_images)) * 100))\n",
    "#         print(\"Itera: {} Test Error: {:4f} Accuracy: {:.4f}%\".format(itera, \n",
    "#                     test_error/len(testing_images), (test_correct/len(testing_images)) * 100))\n",
    "    \n",
    "    train_losses.append(train_error)\n",
    "#     test_losses.append(test_error)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, \"r\", label=\"Train Loss\")\n",
    "# plt.plot(test_losses, \"b\", label=\"Test Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, Our model is not generalized well. We may try with using dropout, larger trainsets, mini-batches to generalize our model well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_idx):\n",
    "        img = training_images[img_idx]\n",
    "        label = training_labels[img_idx].reshape((10, 1))\n",
    "        \n",
    "        layer_0 = img.reshape((input_image_depth, input_image_height, input_image_width))\n",
    "        \n",
    "        layer_1 = convolution(layer_0, kernel_1)\n",
    "\n",
    "        layer_2 = convolution(layer_1, kernel_2)\n",
    "        layer_2_flattened = layer_2.reshape((-1, 1))\n",
    "        \n",
    "        layer_3 = tanh(weight_layer_2_layer_3.T.dot(layer_2_flattened))\n",
    "    \n",
    "        \n",
    "        \n",
    "        final_output = weight_layer_3_output.T.dot(layer_3)\n",
    "        final_outut = softmax(final_output)\n",
    "        plt.title(\"Actual: {} Predicted: {}\".format(index_to_label_name[np.argmax(label)], index_to_label_name[np.argmax(final_output)]))\n",
    "        plt.imshow(img.reshape((28,28)), cmap=\"gray\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(img_idx = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(img_idx = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
